```
# 评估大模型Agents的准确性和可靠性

## AgentBeach测试框架
- 由清华大学、俄亥俄州立大学和加州大学伯克利分校设计
- 包括8个环境：OS、DB、KG、DCG、LTP、HH、WS、WB
- 帮助了解Agent在不同环境和任务中的表现

## ToolEmu安全测试
- 基于仿真框架的安全测试工具
- 模拟多样化工具集，检测LLM-Base Agent表现
- 自动化发现真实世界中的故障场景
- 包括对抗性仿真器和自动安全评估器

## Agent执行轨迹评估
- 观察Agent任务执行过程的动作和响应
- 评估逻辑、效率和工具选择
- 强调全面性、逻辑性、效率性和正确性

## 分类指标与相关性指标
- 使用召回率（Recall）和精确率（Precision）
- 评估Cohen's Kappa、Kendall's Tau、Spearman's rho等评分一致性
- 分析模型处理分类问题的表现

## 直接打分与成对比较
- 直接打分：评估单个回答的准确性
- 成对比较：比较模型与人类回答的差异
- 帮助评估模型的准确性和可靠性

## 参考标准评估
- 将模型回答与参考答案对比
- 评估回答匹配程度
- 适用于特定任务表现评估
```